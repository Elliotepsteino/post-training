@misc{cheng2024dated,
  title         = {Dated Data: Tracing Knowledge Cutoffs in Large Language Models},
  author        = {Jeffrey Cheng and Marc Marone and Orion Weller and Dawn Lawrie and Daniel Khashabi and Benjamin Van Durme},
  year          = {2024},
  eprint        = {2403.12958},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2403.12958}
}
@misc{ellahib2026temporalleakage,
  title         = {Temporal Leakage in Search-Engine Date-Filtered Web Retrieval: A Case Study from Retrospective Forecasting},
  author        = {Ali El Lahib and Ying-Jieh Xia and Zehan Li and Yuxuan Wang and Xinyu Pi},
  year          = {2026},
  eprint        = {2602.00758},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://www.arxiv.org/abs/2602.00758}
}
@article{dhingra2022timeaware,
  title   = {Time-Aware Language Models as Temporal Knowledge Bases},
  author  = {Bhuwan Dhingra and Jeremy R. Cole and Julian Martin Eisenschlos and Daniel Gillick and Jacob Eisenstein and William W. Cohen},
  journal = {Transactions of the Association for Computational Linguistics},
  year    = {2022},
  url     = {https://aclanthology.org/2022.tacl-1.15/}
}
@inproceedings{zhu2025outdated,
  title     = {Is Your LLM Outdated? A Deep Look at Temporal Generalization},
  author    = {Chenghao Zhu and Nuo Chen and Yufei Gao and Yunyi Zhang and Prayag Tiwari and Benyou Wang},
  booktitle = {NAACL 2025 (Long Papers)},
  year      = {2025},
  url       = {https://aclanthology.org/2025.naacl-long.381/}
}
@misc{shi2024detectpretrain,
  title         = {Detecting Pretraining Data from Large Language Models},
  author        = {Weijia Shi and Anirudh Ajith and Mengzhou Xia and Yangsibo Huang and Daogao Liu and Terra Blevins and Danqi Chen and Luke Zettlemoyer},
  year          = {2024},
  eprint        = {2310.16789},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2310.16789}
}
@misc{golchin2024timetravel,
  title         = {Time Travel in LLMs: Tracing Data Contamination in Large Language Models},
  author        = {Shahriar Golchin and Mihai Surdeanu},
  year          = {2024},
  eprint        = {2308.08493},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2308.08493}
}
@misc{golchin2025dcq,
  title         = {Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models},
  author        = {Shahriar Golchin and Mihai Surdeanu},
  year          = {2025},
  eprint        = {2311.06233},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2311.06233}
}
@inproceedings{dong2024generalization,
  title     = {Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models},
  author    = {Yihong Dong and Xue Jiang and Huanyu Liu and Zhi Jin and Bin Gu and Mengfei Yang and Ge Li},
  booktitle = {Findings of ACL 2024},
  year      = {2024},
  url       = {https://aclanthology.org/2024.findings-acl.716/}
}
@misc{li2024taskcontam,
  title         = {Task Contamination: Language Models May Not Be Few-Shot Anymore},
  author        = {Changmao Li and Jeffrey Flanigan},
  year          = {2024},
  eprint        = {2312.16337},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2312.16337}
}
@inproceedings{zhu2024cleaneval,
  title     = {CLEAN–EVAL: Clean Evaluation on Contaminated Large Language Models},
  author    = {Wenhong Zhu and Hongkun Hao and Zhiwei He and Yunze Song and Yumeng Zhang and Hanxu Hu and Yiran Wei and Rui Wang and Hongyuan Lu},
  booktitle = {Findings of NAACL 2024},
  year      = {2024},
  url       = {https://aclanthology.org/2024.findings-naacl.53/}
}
@inproceedings{zhu2024itd,
  title     = {Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation},
  author    = {Qin Zhu and Qingyuan Cheng and Runyu Peng and Xiaonan Li and Tengxiao Liu and Ru Peng and Xipeng Qiu and Xuanjing Huang},
  booktitle = {Findings of EMNLP 2024},
  year      = {2024},
  url       = {https://aclanthology.org/2024.findings-emnlp.532/}
}
@misc{cheng2025surveycontam,
  title         = {A Survey on Data Contamination for Large Language Models},
  author        = {Yuxing Cheng and Yi Chang and Yuan Wu},
  year          = {2025},
  eprint        = {2502.14425},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2502.14425}
}
@inproceedings{loureiro2022timelms,
  title     = {TimeLMs: Diachronic Language Models from Twitter},
  author    = {Daniel Loureiro and Francesco Barbieri and Leonardo Neves and Luis Espinosa Anke and Jose Camacho-Collados},
  booktitle = {ACL 2022 System Demonstrations},
  year      = {2022},
  url       = {https://aclanthology.org/2022.acl-demo.25/}
}
@misc{faro2025timoe,
  title         = {TiMoE: Time-Aware Mixture of Language Experts},
  author        = {Robin Faro and Dongyang Fan and Tamar Alphaidze and Martin Jaggi},
  year          = {2025},
  eprint        = {2508.08827},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2508.08827}
}
@misc{chen2021timeqa,
  title        = {A Dataset for Answering Time-Sensitive Questions},
  author       = {Wenhu Chen and Xinyi Wang and William Yang Wang},
  year         = {2021},
  howpublished = {\url{https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/1f0e3dad99908345f7439f8ffabdffc4-Abstract-round2.html}},
  note         = {NeurIPS 2021 Datasets and Benchmarks}
}
@inproceedings{zhang2021situatedqa,
  title     = {SituatedQA: Incorporating Extra-Linguistic Contexts into QA},
  author    = {Michael J.Q. Zhang and Eunsol Choi},
  booktitle = {EMNLP 2021},
  year      = {2021},
  url       = {https://aclanthology.org/2021.emnlp-main.586/}
}
@inproceedings{ning2020torque,
  title     = {TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions},
  author    = {Qiang Ning and Hao Wu and Rujun Han and Nanyun Peng and Matt Gardner and Dan Roth},
  booktitle = {EMNLP 2020},
  year      = {2020},
  url       = {https://aclanthology.org/2020.emnlp-main.88/}
}
@inproceedings{uddin2025unseentimeqa,
  title     = {UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs’ Memorization},
  author    = {Md Nayem Uddin and Amir Saeidi and Divij Handa and Agastya Seth and Tran Cao Son and Eduardo Blanco and Steven R. Corman and Chitta Baral},
  booktitle = {ACL 2025},
  year      = {2025},
  url       = {https://aclanthology.org/2025.acl-long.94/}
}
@misc{margatina2023dynamictemplama,
  title         = {Dynamic Benchmarking of Masked Language Models on Temporal Concept Drift with Multiple Views},
  author        = {Katerina Margatina and Shuai Wang and Yogarshi Vyas and Neha Anna John and Yassine Benajiba and Miguel Ballesteros},
  year          = {2023},
  eprint        = {2302.12297},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2302.12297}
}
@misc{mousavi2024dyknow,
  title         = {DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs},
  author        = {Seyed Mahed Mousavi and Simone Alghisi and Giuseppe Riccardi},
  year          = {2024},
  eprint        = {2404.08700},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2404.08700}
}
@inproceedings{anonymous2026tdbench,
  title     = {Harnessing Temporal Databases for Systematic Evaluation of Factual Time-Sensitive Question-Answering in LLMs},
  author    = {Anonymous authors},
  booktitle = {Under review (ICLR 2026 submission on OpenReview)},
  year      = {2026},
  url       = {https://openreview.net/forum?id=W7RNxsTKKZ}
}

@misc{london_llm_1800,
  author = {Grigorian, Hayk and Yaghoobian, Hamed},
  title = {Historic London English (1800–1875)},
  year = {2025},
  publisher = {Hugging Face},
  howpublished = {\url{https://huggingface.co/datasets/postgrammar/london-llm-1800}}
}