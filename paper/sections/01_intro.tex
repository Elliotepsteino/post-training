\section{Introduction}
Given a text sample, a natural high-level problem is to assign a year label that reflects its temporal compatibility, which depends on the downstream application.
Such labels can be used for dataset auditing, evaluation, and data selection.

In this paper we focus on the year-assignment task needed to bucket training data for an LLM whose knowledge is restricted to a cutoff year $\tau$ (e.g., backtesting NLP-based models and what-if scenario analysis under historical knowledge constraints).
Supporting such models requires time-bucketed data so that a model trained through year $\tau$ only sees samples that are admissible at $\tau$.

\paragraph{Key considerations for year bucketing.}
We highlight three key considerations.
\begin{enumerate}
    \item Explicit facts should become available on time.
    If an explicit fact became publicly knowable in year $y$ (e.g., a product launch), then a model trained through year $y$ should be able to state it.
    \item Implicit social knowledge should match contemporaneous public understanding.
    For example, consider the implicit statement ``There has been a lot of talk about global warming on the news.''
    Media coverage and public salience increased over time, so it is hard to pinpoint the earliest year when that sentence could have been said.
    If a model trained only through the 1970s were highly knowledgeable and concerned about climate change in a way that matches much later discourse, this would be viewed as temporal leakage.
    \item Explicit and implicit cues correspond to different temporal notions.
    For explicit entities, the earliest admissible time is often close to a single year.
    For implicit entities, admissibility is ambiguous, so the earliest admissible time is naturally distributed over a range.
    These distinctions align with different notions of time: \emph{event time} (when a fact becomes true), \emph{epistemic availability time} (when a well-informed observer could know it), \emph{authorship or publication time} (when the text was written), and \emph{stylistic or discourse time} (when the language and norms are typical).
    For downstream applications, \emph{epistemic availability time} is the best-aligned notion: an event may be known to insiders months or years before it becomes public, but a useful time-bounded LLM should not depend on insider knowledge.
    Likewise, \emph{authorship or publication time} can be misleading: a contemporary expert can write text that is fully consistent with 19th-century knowledge and style, in which case the relevant label is when the text could have been written rather than when it actually was.
\end{enumerate}

\paragraph{Task.}
Let $x$ be a text sample.
Let $P_{x,\mathrm{gt}}(t)$ denote the (latent) distribution over earliest times $t$ such that the text could be produced without relying on unavailable or speculative knowledge at time $t$, as judged by a well-informed observer.
Given $x$, predict the $\alpha$-quantile of $P_{x,\mathrm{gt}}$:
\[
t_\alpha(x)
\;=\;
\inf\Bigl\{ t \;:\; \Pr_{T \sim P_{x,\mathrm{gt}}}\!\bigl(T \le t\bigr) \ge \alpha \Bigr\}.
\]

\paragraph{Interpretation.}
A fraction $\alpha$ of informed observers would judge time $t_\alpha(x)$ (or earlier) to be admissible.
Times earlier than $t_\alpha(x)$ lie in the premature tail.

\paragraph{Temporal semantics and probabilistic formulation.}
Document dating methods implicitly target authorship time by estimating $\operatorname*{arg\,max}_{t} p(T=t \mid x)$.
This is a different goal from ours: we care about which knowledge cutoff makes the text admissible, not when it was authored.
Our setting is fully digital, so classical forensic signals (e.g., ink age, paper composition, and physical provenance) are unavailable, and we emphasize scalability for large corpora.
In contrast, training time-bounded LLMs requires reasoning about admissibility: for which times a text can be produced without relying on future or speculative knowledge.

Formally, let $p_x(t)$ denote a latent distribution over admissible times for a text $x$, reflecting epistemic uncertainty rather than authorship likelihood.
One option is to collapse this distribution to an earliest admissible time
\[
t_{\min}(x)
\;=\;
\inf\Bigl\{ t \;:\; \Pr_{T \sim p_x}\!\bigl(T \le t\bigr) > 0 \Bigr\},
\]
yielding a conservative lower bound that enforces strict temporal monotonicity and minimizes leakage.
However, this approach discards uncertainty and tends to assign temporally ambiguous or timeless text to early years.
An alternative is to treat $x$ as compatible with a distribution over admissible times, including it in training for a model at time $t$ with weight proportional to $\Pr_{T \sim p_x}(T \le t)$.
This preserves uncertainty and yields smoother knowledge accumulation, at the cost of weaker worst-case leakage guarantees.
These two formulations reflect different probabilistic assumptions (pointwise lower bounds versus cumulative compatibility) and should be chosen explicitly based on the intended use of the model.

\paragraph{Applications.}
Synchronos LLMs are models trained on data restricted to specific year ranges.
Potential applications include finance backtesting to avoid look-ahead bias, historical analysis with time-bounded knowledge, auditing model behavior under controlled knowledge cutoffs, and reproducible evaluations that fix the temporal scope of training data.
