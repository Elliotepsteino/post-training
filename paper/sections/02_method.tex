\section{Method}

\paragraph{Task.}
Let $x$ be a text sample.
Let $p_x(t)$ denote the (latent) distribution over earliest times $t$ such that the text could be produced without relying on unavailable or speculative knowledge at time $t$, as judged by a well-informed public observer.
Given $x$, predict the $\alpha$-quantile of $p_x$:
\[
t_\alpha(x)
\;=\;
\inf\Bigl\{ t \;:\; \Pr_{T \sim p_x}\!\bigl(T \le t\bigr) \ge \alpha \Bigr\}.
\]

\paragraph{Interpretation.}
A fraction $\alpha$ of observers would judge time $t_\alpha(x)$ (or earlier) to be admissible.
Times earlier than $t_\alpha(x)$ lie in the premature tail.

\paragraph{Entity types.}
We group entities into three categories: explicit, implicit, and timeless.
We define \emph{explicit} entities as those with a single right admissible year under our notion of epistemic availability (e.g., a concrete product launch year), so their admissible-time uncertainty is negligible at year granularity and can be treated as a point mass.
For example, ``I just bought the first iPhone that was released last week'' is explicit: the admissible time concentrates on the release year of the first iPhone, so $p_x(t)$ is effectively a delta mass on that year.
We define \emph{implicit} entities as those without a single right year, where admissibility depends on diffuse public salience and can vary across observers.
For example, ``There has been a lot of talk about global warming on the news'' is implicit: coverage has intensified over decades, so it is difficult to pinpoint the earliest year when the statement becomes true. As a result, the earliest-admissible time is itself distributed, and our goal is to return a sample from that distribution.
Finally, \emph{timeless} samples contain no time-anchored entity.
%We do not use a database agent because there is no internal database to cross-reference in this pipeline.

\paragraph{Operational pipeline.}
\Cref{fig:filtering-flow} summarizes our end-to-end implementation.
We operationalize the method as a five-step agent pipeline:
\begin{enumerate}
    \item Entity Extraction Agent: extract time-anchored entities from the question and answer bundle.
    \item Reasoner Agent: assign a best-estimate year and a 95\% confidence interval per entity.
    \item Search Query Agent: generate standalone search queries for each entity.
    \item Internet Search Agent: retrieve evidence from search results to ground the estimates.
    \item Reasoner/Synthesizer Agent: update entity confidence intervals based on the evidence and update the overall estimated year using the maximum upper bound across entities.
\end{enumerate}

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[
        node distance=0.7cm and 1.0cm,
        box/.style={rectangle, rounded corners, draw=black, thick, align=center, text width=0.23\textwidth, minimum height=0.85cm},
        phase/.style={rectangle, draw=none, align=center, font=\bfseries},
        note/.style={rectangle, draw=black, thick, align=left, text width=0.23\textwidth, font=\small},
        arrow/.style={-Latex, thick}
    ]
        \node[box] (input) {Raw sample\\(all text)};
        \node[box, right=of input, xshift=0.4cm] (ground) {Entity grounding};
        \node[box, right=of ground, xshift=0.4cm] (agg) {CI Aggregation\\(rank-quantiles / LLM)};

        \node[phase, above=of input] (phase1) {Ingest};
        \path let \p1 = (phase1), \p2 = (ground) in node[phase] (phase2) at (\x2,\y1) {Evidence};
        \path let \p1 = (phase1), \p2 = (agg) in node[phase] (phase3) at (\x2,\y1) {Aggregation};

        \node[note, below=of ground] (ground_note) {extract entities\\build queries\\search evidence\\form CI\\take upper bounds};
        \node[box, below=of agg] (assign) {Final year\\label};

        \draw[arrow] (input) -- (ground);
        \draw[arrow] ([yshift=10pt]ground.east) -- ([yshift=10pt]agg.west);
        \draw[arrow] ([yshift=-10pt]ground.east) -- ([yshift=-10pt]agg.west);
        \node at ($(ground.east)!0.5!(agg.west)$) {\vdots};
        \draw[arrow] (agg) -- (assign);

        \draw[arrow] (ground) -- (ground_note);
        \draw[arrow] (ground_note) -- (ground);

        \draw[decorate, decoration={brace, amplitude=5pt}] (ground.north west) -- (ground.north east);
        \node[font=\small] at ($(ground.north)+(0,0.35)$) {$\times N$};
    \end{tikzpicture}
    \caption{Filtering pipeline for a single sample.
    Evidence extraction and CI aggregation are repeated for $N$ samples, then merged into a final year label.}
    \label{fig:filtering-flow}
\end{figure}

%\paragraph{Why not GLiNER.}
%We evaluated GLiNER for entity extraction, but it is a poor fit for this task.%
%Given fixed categories, it must both detect the entity span and map it to a category, and empirically it misses entities frequently in our data.
%Because missed entities directly induce leakage, we instead use LLMs for the full stack (entity extraction through year estimation), with search-based evidence grounding to improve reliability.
